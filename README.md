# Chat com LLM + PDFs (RAG)

Esse projeto √© um chat em Python usando LLM local (Ollama), com suporte a perguntas
baseadas em PDFs usando RAG.

>  **Aviso**  
> Este README **N√ÉO √© o relat√≥rio final do trabalho**.  
> Ele √© apenas um resumo do que foi feito at√© agora, pensado para facilitar a comunica√ß√£o entre os membros da equipe.  
> Quando o trabalho estiver finalizado, esse documento deve mudar .

---

## üõ† Tecnologias utilizadas

- ![Python](https://img.shields.io/badge/Python-3.10+-blue?logo=python) **Python 3.10+**
- ![LangChain](https://img.shields.io/badge/LangChain-Framework-green?logo=chainlink) **LangChain**
- ![Ollama](https://img.shields.io/badge/Ollama-Local_LLM-black) **Ollama (Llama 3.2)**
- ![ChromaDB](https://img.shields.io/badge/ChromaDB-Vector_Store-purple) **ChromaDB**
- **FlashRank** (reranking no RAG)

---

##  Como executar 

1. Ter **Python 3.10+** instalado  
2. Instalar o **Ollama**
3. Baixar o modelo:
   ```bash
   ollama pull llama3.2
    ```
4. Instale as dependencias:
    ```bash
   pip install -r requirements.txt
    ```
5. Colocar os pdf na pasta de documentos (  usei o pdf da aula como teste [ia2.pdf](documentos/ia2.pdf) )

## üìÅ Estrutura do projeto

- `main.py`  
  Arquivo principal do projeto.  
  Aqui fica:
  - o menu de intera√ß√£o
  - a l√≥gica dos tr√™s tipos de chat
  - a parte de RAG
  - a integra√ß√£o com o modelo local

- `documentos/`  
  Pasta onde devem ser colocados os PDFs.  
  Tudo que estiver aqui √© carregado automaticamente quando o programa inicia.

- `logs_interacao_ia.txt`  
  Arquivo com o registro das intera√ß√µes com IA durante o desenvolvimento.  
  Funciona como um di√°rio t√©cnico, documentando decis√µes, d√∫vidas e ajustes feitos,
  conforme solicitado nas especifica√ß√µes do trabalho .

- `requisitos.txt`  
  Lista das bibliotecas necess√°rias para executar o projeto.

---

##  Como o c√≥digo foi pensado (resumo geral)

O trabalho exigia **tr√™s tipos de chat**, ent√£o o c√≥digo foi estruturado para atender
cada um deles separadamente:

### 1.  Chat simples
- Apenas envia a pergunta diretamente para o modelo
- N√£o usa PDFs
- Serve como base de compara√ß√£o

### 2. Chat com contexto completo (Full Context)
- Junta todo o texto dos PDFs em um √∫nico contexto
- Envia tudo de uma vez para o modelo

### 3. Chat RAG
- Os PDFs s√£o quebrados em pequenos trechos
- Cada trecho vira um embedding
- Um banco vetorial armazena esses embeddings
- Na pergunta, apenas os trechos mais relevantes s√£o usados

Alem disso:
- O banco vetorial √© criado uma √∫nica vez no in√≠cio do programa, evitando lentid√£o a cada nova pergunta.
- O RAG recupera mais trechos do que o necess√°rio e refina essa sele√ß√£o depois

---


## Refer√™ncias

- NETWORK CHUCK. *Build a RAG system with LangChain (from scratch)*. YouTube, 2023.  
  Dispon√≠vel em: https://www.youtube.com/watch?v=E4l91XKQSgw&t=1096s  
  Acesso em: 3 jan. 2026.

- LANGCHAIN. *LangChain Documentation*.  
  Dispon√≠vel em: https://python.langchain.com/  
  Acesso em: 3 jan. 2026.

- LANGCHAIN. *Ollama integration*.  
  Dispon√≠vel em: https://python.langchain.com/docs/integrations/llms/ollama/  
  Acesso em: 3 jan. 2026.

- OLLAMA. *Ollama: run large language models locally*.  
  Dispon√≠vel em: https://ollama.com/  
  Acesso em: 3 jan. 2026.

- LANGCHAIN. *Text embeddings*.  
  Dispon√≠vel em: https://python.langchain.com/docs/concepts/text_embeddings/  
  Acesso em: 3 jan. 2026.

- CHROMA. *Chroma Documentation*.  
  Dispon√≠vel em: https://docs.trychroma.com/  
  Acesso em: 3 jan. 2026.

- FLASHRANK. *FlashRank: fast reranking for retrieval-augmented generation*.  
  Dispon√≠vel em: https://github.com/PrithivirajDamodaran/FlashRank  
  Acesso em: 3 jan. 2026.
